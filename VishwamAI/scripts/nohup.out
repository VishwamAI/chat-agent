2024-06-22 08:53:36.134160: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-22 08:53:36.142188: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-22 08:53:36.239961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-22 08:53:37.697097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-06-22 08:53:40,467 - INFO - Unable to initialize backend 'cuda': 
2024-06-22 08:53:40,467 - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2024-06-22 08:53:40,469 - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
Data type of inputs after conversion: <dtype: 'int32'>
Data type of inputs before embedding layer: <dtype: 'int32'>
Data type of inputs after conversion to int32: <dtype: 'int32'>
Data type of embedded inputs after embedding layer: float32
Data type of inputs before transformer apply: float32
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 126, in to_prng_sequence
    rng = hk.PRNGSequence(rng)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/base.py", line 1046, in __init__
    assert_is_prng_key(key_or_seed)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/base.py", line 1004, in assert_is_prng_key
    raise ValueError(
ValueError: Provided key did not have expected shape and/or dtype: expected=(shape=(2,), dtype=uint32), actual=(shape=(8, 32, 64), dtype=float32)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/train_vishwamai_model.py", line 155, in <module>
    train_model(data_file)
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/train_vishwamai_model.py", line 130, in train_model
    params = transformed_forward.init(init_rng, example_batch)  # Pass the correct arguments
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 166, in init_fn
    params, state = f.init(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 422, in init_fn
    f(*args, **kwargs)
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/train_vishwamai_model.py", line 105, in forward_fn
    logits = model(batch)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/module.py", line 464, in wrapped
    out = f(*args, **kwargs)
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/module.py", line 305, in run_interceptors
    return bound_method(*args, **kwargs)
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/model_architecture.py", line 77, in __call__
    transformer_output = self.transformer.apply(None, embedded_inputs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 183, in apply_fn
    out, state = f.apply(params, None, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 451, in apply_fn
    rng = to_prng_sequence(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 128, in to_prng_sequence
    raise ValueError(
ValueError: Apply must be called with an RNG as the second argument, the required signature is: `apply(params, rng, *a, **k)`. The object was of type <class 'jaxlib.xla_extension.ArrayImpl'>: [[[-0.00957099 -0.0042609   0.00340895 ...  0.00261725 -0.00052745
   -0.00532985]
  [-0.00553907  0.00588879  0.01242392 ...  0.01528201 -0.00297135
    0.01714389]
  [ 0.01099068  0.00933441  0.00521326 ...  0.01280174 -0.00509705
    0.00325228]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[-0.00957099 -0.0042609   0.00340895 ...  0.00261725 -0.00052745
   -0.00532985]
  [-0.00553907  0.00588879  0.01242392 ...  0.01528201 -0.00297135
    0.01714389]
  [ 0.01099068  0.00933441  0.00521326 ...  0.01280174 -0.00509705
    0.00325228]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[ 0.01099068  0.00933441  0.00521326 ...  0.01280174 -0.00509705
    0.00325228]
  [ 0.0070703  -0.00351574 -0.00975035 ... -0.00747036  0.00742292
   -0.00616953]
  [-0.01405564 -0.00126728  0.01278545 ...  0.00584597  0.018904
    0.00264597]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 ...

 [[ 0.00240181  0.00471304  0.00595017 ...  0.00469747  0.0094203
   -0.00659482]
  [ 0.004988    0.01414642  0.01769772 ... -0.00013558  0.00023749
   -0.01101039]
  [ 0.02158225 -0.00600585  0.00033362 ...  0.00391613  0.01691691
   -0.00290484]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[ 0.0187214   0.00175198 -0.00095723 ...  0.0177877   0.00674335
   -0.00305236]
  [ 0.00195901 -0.00964023  0.00302406 ... -0.00480226 -0.01641154
    0.00061692]
  [ 0.00773156  0.01634322 -0.01234329 ...  0.00199128  0.00440729
    0.00442611]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[ 0.0187214   0.00175198 -0.00095723 ...  0.0177877   0.00674335
   -0.00305236]
  [ 0.00195901 -0.00964023  0.00302406 ... -0.00480226 -0.01641154
    0.00061692]
  [ 0.00773156  0.01634322 -0.01234329 ...  0.00199128  0.00440729
    0.00442611]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]]
2024-06-22 09:05:17.888026: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-22 09:05:17.894677: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-22 09:05:17.990534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-22 09:05:19.415439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-06-22 09:05:22,482 - INFO - Unable to initialize backend 'cuda': 
2024-06-22 09:05:22,482 - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
2024-06-22 09:05:22,483 - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
Data type of inputs after conversion: <dtype: 'int32'>
Data type of inputs before embedding layer: <dtype: 'int32'>
Data type of inputs after conversion to int32: <dtype: 'int32'>
Data type of embedded inputs after embedding layer: float32
Data type of inputs before transformer apply: float32
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 126, in to_prng_sequence
    rng = hk.PRNGSequence(rng)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/base.py", line 1046, in __init__
    assert_is_prng_key(key_or_seed)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/base.py", line 1004, in assert_is_prng_key
    raise ValueError(
ValueError: Provided key did not have expected shape and/or dtype: expected=(shape=(2,), dtype=uint32), actual=(shape=(8, 32, 64), dtype=float32)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/train_vishwamai_model.py", line 154, in <module>
    train_model(data_file)
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/train_vishwamai_model.py", line 129, in train_model
    params = transformed_forward.init(init_rng, example_batch)  # Pass the correct arguments
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 166, in init_fn
    params, state = f.init(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 422, in init_fn
    f(*args, **kwargs)
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/train_vishwamai_model.py", line 104, in forward_fn
    logits = model(batch)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/module.py", line 464, in wrapped
    out = f(*args, **kwargs)
  File "/usr/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/module.py", line 305, in run_interceptors
    return bound_method(*args, **kwargs)
  File "/home/ubuntu/chat-agent/VishwamAI/scripts/model_architecture.py", line 77, in __call__
    transformer_output = self.transformer.apply(None, embedded_inputs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 183, in apply_fn
    out, state = f.apply(params, None, *args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 451, in apply_fn
    rng = to_prng_sequence(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/haiku/_src/transform.py", line 128, in to_prng_sequence
    raise ValueError(
ValueError: Apply must be called with an RNG as the second argument, the required signature is: `apply(params, rng, *a, **k)`. The object was of type <class 'jaxlib.xla_extension.ArrayImpl'>: [[[-0.00957099 -0.0042609   0.00340895 ...  0.00261725 -0.00052745
   -0.00532985]
  [-0.00553907  0.00588879  0.01242392 ...  0.01528201 -0.00297135
    0.01714389]
  [ 0.01099068  0.00933441  0.00521326 ...  0.01280174 -0.00509705
    0.00325228]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[-0.00957099 -0.0042609   0.00340895 ...  0.00261725 -0.00052745
   -0.00532985]
  [-0.00553907  0.00588879  0.01242392 ...  0.01528201 -0.00297135
    0.01714389]
  [ 0.01099068  0.00933441  0.00521326 ...  0.01280174 -0.00509705
    0.00325228]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[ 0.01099068  0.00933441  0.00521326 ...  0.01280174 -0.00509705
    0.00325228]
  [ 0.0070703  -0.00351574 -0.00975035 ... -0.00747036  0.00742292
   -0.00616953]
  [-0.01405564 -0.00126728  0.01278545 ...  0.00584597  0.018904
    0.00264597]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 ...

 [[ 0.00240181  0.00471304  0.00595017 ...  0.00469747  0.0094203
   -0.00659482]
  [ 0.004988    0.01414642  0.01769772 ... -0.00013558  0.00023749
   -0.01101039]
  [ 0.02158225 -0.00600585  0.00033362 ...  0.00391613  0.01691691
   -0.00290484]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[ 0.0187214   0.00175198 -0.00095723 ...  0.0177877   0.00674335
   -0.00305236]
  [ 0.00195901 -0.00964023  0.00302406 ... -0.00480226 -0.01641154
    0.00061692]
  [ 0.00773156  0.01634322 -0.01234329 ...  0.00199128  0.00440729
    0.00442611]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]

 [[ 0.0187214   0.00175198 -0.00095723 ...  0.0177877   0.00674335
   -0.00305236]
  [ 0.00195901 -0.00964023  0.00302406 ... -0.00480226 -0.01641154
    0.00061692]
  [ 0.00773156  0.01634322 -0.01234329 ...  0.00199128  0.00440729
    0.00442611]
  ...
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]
  [-0.00297072  0.01160661  0.00799139 ...  0.00880169 -0.00936524
    0.00931786]]]
